{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "import re\n",
                "import nltk\n",
                "\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Carga de archivos de datos"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Debido a que son archivos de texto que no se separan por columnas, no lo abrimos con pandas para que no sea un dataset sino que solo leemos todo el archivo, linea por linea."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "f_blogs = open('en_US/en_US.blogs.txt', 'r')\n",
                "blogs = f_blogs.read()\n",
                "f_blogs.close()\n",
                "blogs = word_tokenize(blogs)\n",
                "\n",
                "f_news = open('en_US/en_US.news.txt', 'r')\n",
                "news = f_news.read()\n",
                "f_news.close()\n",
                "news = word_tokenize(news)\n",
                "\n",
                "f_twitter = open('en_US/en_US.twitter.txt', 'r')\n",
                "tweets = f_twitter.read()\n",
                "f_twitter.close()\n",
                "tweets = word_tokenize(tweets)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Limpieza de datos"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Pasamos todo el texto a minusculas por medio de un for en donde se pasa a minuscula cada palabra en el archivo de texto."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "blogs = [word.lower() for word in blogs]\n",
                "news = [word.lower() for word in news]\n",
                "tweets = [word.lower() for word in tweets]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Eliminamos los caractéres especiales por medio de expresiones regulares con la libreria re de python. Con esta expresión regular quitamos todo aquello que no sea letras o numeros de manera que eliminamos tambien los signos de puntuación."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "special_characters = re.compile(\"[^A-Za-z0-9]+\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "blogs = [special_characters.sub(r' ', word) for word in blogs]\n",
                "news = [special_characters.sub(r' ', word) for word in news]\n",
                "tweets = [special_characters.sub(r' ', word) for word in tweets]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Eliminamos las url, tambien con expresiones regulares verificando que el inicio sea con http o https."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))', flags=re.MULTILINE)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "blogs = [url_pattern.sub(r' ', word) for word in blogs]\n",
                "news = [url_pattern.sub(r' ', word) for word in news]\n",
                "tweets = [url_pattern.sub(r' ', word) for word in tweets]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Eliminamos los caracteres especiales por medio de una expresión regular (con ayuda de la libreria que tra python de las expresiones regulares y un for para validar cada palabra. \n",
                "### Lo eliminamos por medio de los patterns de emojis (u\"\\U0001F600-\\U0001F64F), simbolos (u\"\\U0001F300-\\U0001F5FF), simbolos de mapa y transporte (u\"\\U0001F680-\\U0001F6FF\"), y banderas (u\"\\U0001F1E0-\\U0001F1FF\")"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "emoji_pattern = re.compile(\"[\"\n",
                "        u\"\\U0001F600-\\U0001F64F\"\n",
                "        u\"\\U0001F300-\\U0001F5FF\" \n",
                "        u\"\\U0001F680-\\U0001F6FF\"\n",
                "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
                "                           \"]+\", flags=re.UNICODE)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "blogs = [emoji_pattern.sub(r'', word) for word in blogs]\n",
                "news = [emoji_pattern.sub(r'', word) for word in news]\n",
                "tweets = [emoji_pattern.sub(r'', word) for word in tweets]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Eliminamos las stopword con ayuda de la libreria de NLTK y su modulo de corpus"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "# nltk.download('stopwords')\n",
                "# nltk.download('punkt')\n",
                "\n",
                "blogs = [word for word in blogs if not word in stopwords.words()]\n",
                "news = [word for word in news if not word in stopwords.words()]\n",
                "tweets = [word for word in tweets if not word in stopwords.words()]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}